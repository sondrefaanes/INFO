{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637cc60d"
      },
      "source": [
        "# Reading plain text from the web\n",
        "\n",
        "by Koenraad De Smedt at UiB"
      ],
      "id": "637cc60d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lME03Clev2yx"
      },
      "source": [
        "---\n",
        "There is a lot of textual material on the web that can be processed. But there are also a lot of different text encodings and formats. This notebook will deal with the simplest case, namely, reading a webpage consisting of plain Unicode text.\n",
        "\n",
        "---"
      ],
      "id": "lME03Clev2yx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Getting the text\n",
        "\n",
        "We need to import the `requests` module that can open a webpage based on its url."
      ],
      "metadata": {
        "id": "yF_s9HMltE3D"
      },
      "id": "yF_s9HMltE3D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHlJrSVxzkVt"
      },
      "source": [
        "import requests"
      ],
      "id": "hHlJrSVxzkVt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Asb7R4_r_d"
      },
      "source": [
        "Here is an example URL that points to a plain text on the web. You can open it in a new tab in the browser to check that it contains plain text."
      ],
      "id": "c3Asb7R4_r_d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzpLxopFAnwo"
      },
      "source": [
        "emma_url = 'https://raw.githubusercontent.com/fbkarsdorp/python-course/master/data/austen-emma.txt'"
      ],
      "id": "IzpLxopFAnwo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX3xMtd2Aqie"
      },
      "source": [
        "The function `requests.get` opens a webpage based on the url. There are several kinds of information in the response, but here we are only interested in getting the textual content as a Unicode string by means of `.text`.\n",
        "\n",
        "In this example, we take only a 800 characters because the whole text is too big to display."
      ],
      "id": "lX3xMtd2Aqie"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UONEr_USJnsJ"
      },
      "source": [
        "emma_text = requests.get(emma_url).text[:800]\n",
        "print(emma_text)"
      ],
      "id": "UONEr_USJnsJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ydjfB3q3Zv"
      },
      "source": [
        "# 1. Computing tokens, types and frequencies\n",
        "\n",
        "Now that we have plain text, we can further process it. We use the `nltk` module which provides some useful text manipulation and counting functions."
      ],
      "id": "L3ydjfB3q3Zv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoHSmzUkNOeM"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize, FreqDist"
      ],
      "id": "PoHSmzUkNOeM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgquohDFX420"
      },
      "source": [
        "Make a list of word tokens from the lowercased text."
      ],
      "id": "YgquohDFX420"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvA9hcIWrU2l"
      },
      "source": [
        "emma_tokens = word_tokenize(emma_text.lower())\n",
        "print(emma_tokens[:30])"
      ],
      "id": "JvA9hcIWrU2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The set of types can also be called the vocabulary of the text."
      ],
      "metadata": {
        "id": "Jaak2XJb8W8x"
      },
      "id": "Jaak2XJb8W8x"
    },
    {
      "cell_type": "code",
      "source": [
        "emma_types = set(emma_tokens)\n",
        "print(emma_types)"
      ],
      "metadata": {
        "id": "75zklBR-8VOk"
      },
      "id": "75zklBR-8VOk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amU3kxVDsM3T"
      },
      "source": [
        "We can compute the lexical variation by dividing the number of types by the number of tokens. The larger this number, the more varied use of words. The lower this number, the more repetition of words. For a very short text, this number doesn't mean so much."
      ],
      "id": "amU3kxVDsM3T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FeSdAKCsLwt"
      },
      "source": [
        "len(emma_types) / len(emma_tokens)"
      ],
      "id": "_FeSdAKCsLwt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA0ynta7cqbL"
      },
      "source": [
        "Let's define a function for lexical variation based on this proportion."
      ],
      "id": "aA0ynta7cqbL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST4VaNQFcIGQ"
      },
      "source": [
        "def lexical_variation (text):\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  types = set(tokens)\n",
        "  return len(types) / len(tokens)\n",
        "\n",
        "lexical_variation(emma_text)"
      ],
      "id": "ST4VaNQFcIGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a frequency distribution."
      ],
      "metadata": {
        "id": "oKloHdy53Tnd"
      },
      "id": "oKloHdy53Tnd"
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist = FreqDist(emma_tokens)\n",
        "freq_dist['emma'] # assume tokens are all lowercase"
      ],
      "metadata": {
        "id": "rU8OY5dG_t3H"
      },
      "id": "rU8OY5dG_t3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Streaming line by line\n",
        "\n",
        "Instead of reading the whole text of the webpage into a string, which may be very long, it is also possible to read and process *streamed* content line by line. The stream works like a generator, so that only as many lines are read as the program asks for. The code in the following cell reads and prints the first 20 lines only and also prints the counter.\n",
        "\n",
        "By default, `iter_lines` gives raw strings, so we need to tell it to decode each line into text."
      ],
      "metadata": {
        "id": "Agwt6dOy9sfk"
      },
      "id": "Agwt6dOy9sfk"
    },
    {
      "cell_type": "code",
      "source": [
        "emma_stream = requests.get(emma_url, stream=True)\n",
        "linestream = emma_stream.iter_lines(decode_unicode=True)\n",
        "for n in range(20):\n",
        "  print(n, next(linestream))"
      ],
      "metadata": {
        "id": "pE9W_3byxCjz"
      },
      "id": "pE9W_3byxCjz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an alternative way of reading and printing 20 lines. We zip a range of numbers and a stream of lines. Again, this is very efficient because the zip is limited to 20 lines that are are read."
      ],
      "metadata": {
        "id": "R1okFZMFxbT-"
      },
      "id": "R1okFZMFxbT-"
    },
    {
      "cell_type": "code",
      "source": [
        "emma_stream = requests.get(emma_url, stream=True)\n",
        "for n, line in zip(range(20), emma_stream.iter_lines(decode_unicode=True)):\n",
        "  print(n, line)"
      ],
      "metadata": {
        "id": "Wddpi1-kqiqg"
      },
      "id": "Wddpi1-kqiqg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the text has some empty lines. Suppose we want to read 20 lines and print only non-empty lines, we add a condition with `if`."
      ],
      "metadata": {
        "id": "U569Pkj1okyK"
      },
      "id": "U569Pkj1okyK"
    },
    {
      "cell_type": "code",
      "source": [
        "emma_stream = requests.get(emma_url, stream=True)\n",
        "for n, line in zip(range(20), emma_stream.iter_lines(decode_unicode=True)):\n",
        "  if line: \n",
        "    print(n, line)"
      ],
      "metadata": {
        "id": "_yWp7lXypRfN"
      },
      "id": "_yWp7lXypRfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous counts 20 lines read, not 20 lines written. Suppose we want to print 20 non-empty lines, then we should use a counter that is increased only after we know we have a non-empty line."
      ],
      "metadata": {
        "id": "QtT_mbcOsfa0"
      },
      "id": "QtT_mbcOsfa0"
    },
    {
      "cell_type": "code",
      "source": [
        "emma_stream = requests.get(emma_url, stream=True)\n",
        "linestream = emma_stream.iter_lines(decode_unicode=True)\n",
        "printed = 0\n",
        "while printed < 20:\n",
        "  line = next(linestream)\n",
        "  if line:\n",
        "    print(printed, line)\n",
        "    printed += 1"
      ],
      "metadata": {
        "id": "xjDI9xBGtdd3"
      },
      "id": "xjDI9xBGtdd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Q3GxHUKcyA"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1.  Read the *full* text of Emma into Python. Do not print the whole text, because it is too long, but lowercase it, tokenize it and compute the lexical variation.\n",
        "\n",
        "2.  Given the list `tokens` and the frequency distribution `freq_dist`, print the frequencies of *she* and *he*. Also, compute the relative frequency of these words per million words. Optionally, plot both relative frequencies together in a barplot.\n",
        "\n",
        "3.  Extend the code for reading non-empty lines from Emma so that two counters are printed, one that counts the lines read and another that counts the non-empty lines printed.\n",
        "\n",
        "4.  Find a large word list online with one word on each line. Iterate over its lines and print only the lines that are palindromes. Reuse the palindrome function from the earlier notebook about palindromes. The following are possible URLs for a large English word list. Alternatively, you can look for a list in another language.\n",
        "\n",
        " *   http://wiki.puzzlers.org/pub/wordlists/unixdict.txt\n",
        " *   https://raw.githubusercontent.com/quinnj/Rosetta-Julia/master/unixdict.txt\n",
        " *   https://searchcode.com/codesearch/raw/29038705/\n",
        "\n",
        "5.  Supposed you need help in solving a crossword puzzle. Write a function `search_words` that iterates an online word list, as suggested above, and prints all lines matching a given regex. For instance, `search_words('^[db]a...$')` will look for five-letter words starting with *d* or *b* followed by *a*. You may want to limit the number of words that are printed because there could be many.\n",
        "\n",
        "6.  (optional) Also using an online word list as above, use a comprehension that includes all words longer than, for instance, 15 characters."
      ],
      "id": "Z1Q3GxHUKcyA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcTJuVxtahxS"
      },
      "source": [
        "## Notes\n",
        "\n",
        "1.  See the [documentation of requests](https://docs.python-requests.org/en/master/user/quickstart/) if you need more possibilities to access webpages.\n",
        "In case a webpage is encoded in anything else than UTF-8, then instead of `.text`, one can also use `.content.decode(encoding)`,  which gets the content and interprets that according to the given encoding, for instance `.content.decode('cp1252')`\n",
        "\n",
        "2.  In some versions of Python on MacOS, the use of the *requests* module may cause an error about certificates. If that is the case, you can run the following command on MacOS.\n",
        "\n",
        "><img src=\"https://git.app.uib.no/desmedt/teaching/-/raw/main/Install-certificates.png\" alt = \"slicing\" width = 240px>"
      ],
      "id": "DcTJuVxtahxS"
    }
  ]
}