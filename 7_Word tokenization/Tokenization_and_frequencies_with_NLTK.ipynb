{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPdpx8NFPs2o"
      },
      "source": [
        "# Word tokenization and frequencies with NLTK\n",
        "\n",
        "by Koenraad De Smedt at UiB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk_xz9xPZNpc"
      },
      "source": [
        "---\n",
        "\n",
        "This notebook will introduce [NLTK](https://www.nltk.org/), a Natural Language ToolKit. This notebook will show how to do the following with this toolkit:\n",
        "\n",
        "1.  How to *word-tokenize* a text, i.e. make a list of word tokens extracted from a string (including tokens for punctuation).\n",
        "2.  How to compute the *types*, i.e. the set of unique tokens.\n",
        "3.  How to make a *frequency list* of tokens and find how many times a token occurs in the text.\n",
        "\n",
        "In later notebooks, these techniques will be applied to larger texts read from the Web.\n",
        "\n",
        "For those who want to know more on NLTK, there is an [online book](https://www.nltk.org/book/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example string for a piece of text."
      ],
      "metadata": {
        "id": "VDqhINWeTi1H"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z3h9dvpPrKT"
      },
      "source": [
        "story ='''Once upon a time, there was a princess called Buttercup. She had a\n",
        "farm-hand called Westley; whenever she tells him to do something, he always\n",
        "answers: \"As you wish.\" At first she didn't realize he loves her, but\n",
        "eventually she realizes it and she loves him too! Westley leaves to seek his\n",
        "fortune overseas so they can marry. When his ship is attacked by the Dread\n",
        "Pirate Roberts, who is infamous for never leaving survivors, Westley is\n",
        "presumed dead...'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvY_xhTaO-A8"
      },
      "source": [
        "The NLTK module provides several useful functions for manipulating text. See the [documentation](https://www.nltk.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c3cyDgjN_t1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize, FreqDist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbshcHjaOOQE"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "NLTK provides the function `word_tokenize` which extracts all tokens and returns a list. This tokenizer is somewhat more sophisticated than the simple tokenizer from an earlier notebook. Hyphenated words are kept together. Punctuation is split off and tokens for punctuation are included in the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dpX9ujKOZkI"
      },
      "source": [
        "tokens = word_tokenize(story)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jatenuHPz3o"
      },
      "source": [
        "Use `casefold` if you want to convert everything to lowercase. This may have advantages and disadvantages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMv9LlEIOw4d"
      },
      "source": [
        "tokens = word_tokenize(story.casefold())\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzcDQsvXS9S4"
      },
      "source": [
        "Make the vocabulary, i.e. the word types, by computing the set of unique tokens in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz6AjoqoOcYM"
      },
      "source": [
        "vocab = set(tokens)\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQYb8dWFhdIX"
      },
      "source": [
        "Print the length of the text and the length of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk52LiAOhirx"
      },
      "source": [
        "print(len(tokens), len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3iBcH5-vmx"
      },
      "source": [
        "Make a list of types with more than four characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GePsryfNHX-4"
      },
      "source": [
        "[word for word in vocab if len(word) > 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxK9tnlrpjSj"
      },
      "source": [
        "## Word frequencies\n",
        "\n",
        "Make a frequency distribution of the tokens. This produces a kind of *counter*, which is a special *dict* in which each token is associated with its frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBiMpZolpaiM"
      },
      "source": [
        "freq_dist = FreqDist(tokens)\n",
        "freq_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv4070vDprKx"
      },
      "source": [
        "We can find the frequency by using the token as a key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwU1s07QpxvJ"
      },
      "source": [
        "freq_dist['she']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWpizjEvS2Lo"
      },
      "source": [
        "Compute frequencies sorted by decreasing numbers. This produces a list of tuples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIYQvxPLTU-m"
      },
      "source": [
        "freq_list = freq_dist.most_common()\n",
        "freq_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ZFL53VgFaV"
      },
      "source": [
        "Print the nine most common tokens with their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pzk5ULxgA7j"
      },
      "source": [
        "for (token, freq) in freq_dist.most_common(9):\n",
        "  print(freq, ':', token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZCYKL37_a1g"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1.   Compute the lexical variation, i.e. the ratio of types to tokens.\n",
        "2.   Print the nine most common tokens with their frequencies, but also print the length of each token on the same line.\n",
        "3.   Instead of printing in the previous exercise, use a comprehension to make a list of triples containing the frequency, the token and the token's length.\n",
        "4.   Print the five most common tokens with at least three characters. The easiest is to first use a comprehension that includes tokens with at least three characters, and then make a frequency distribution of that list.\n",
        "5.   (optional) For some purposes, one wants a list containing only words, excluding tokens that consist of just punctuation marks. What needs to be done to keep only words? Tip: if you `import string`, you get the variable `string.punctuation` which has all punctuation marks, so you can write a function that checks if every character of a token is also in `string.punctuation`, and if so, it is not kept.\n",
        "6.   (optional) NLTK can also provide a list of stopwords, see the cell below. Compute the set of words in `vocab` which are not stopwords. You can use `-` for set difference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english')[:12])"
      ],
      "metadata": {
        "id": "vnZdWumYbDeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}