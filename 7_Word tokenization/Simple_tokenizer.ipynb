{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IecPp9qJgO5s"
      },
      "source": [
        "# A simple word tokenizer\n",
        "\n",
        "by Koenraad De Smedt at UiB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1G9WoZkhK7K"
      },
      "source": [
        "---\n",
        "Word *tokenizing* (or *tokenization*) is the process of splitting text in units called *tokens*. Each word is a token. Spaces are disregarded. Punctuation may either be disregarded or may also be recognized as tokens.\n",
        "\n",
        "Tokenization is often a useful first step in working with digital text. At the same time, some *normalization* of the text may be performed. Normalization may include case folding, error correction and *lemmatization* (reducing words to their dictionary form).\n",
        "\n",
        "Based on a list of tokens, one can easily compute the *set of types*, i.e. unique tokens.\n",
        "\n",
        "This notebook shows how to:\n",
        "\n",
        "1.  Define a simple word tokenizer based on a regular expression\n",
        "2.  Refine the tokenizer depending on what is considered a word\n",
        "3.  Compute the types, i.e. the set of unique tokens.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple word tokenizer can be made by splitting a string, using any sequence of non-word characters as a separator."
      ],
      "metadata": {
        "id": "Jk_fsxih-EQ5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdULb4QIhHgm"
      },
      "source": [
        "import re\n",
        "\n",
        "def word_tokens (text):\n",
        "  return re.split(r'\\W+', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_P8Ro4zhs5K"
      },
      "source": [
        "\n",
        "Let’s make a string with a short example text (based on [a popular film](https://g.co/kgs/g3veEe) script), and test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFwRfPa2jmwf"
      },
      "source": [
        "story ='''Once upon a time, there was a princess called Buttercup. She\n",
        "had a farm-hand called Westley; whenever she tells him to do something,\n",
        "e.g., he always answers: \"As you wish.\" At first she didn't realize he \n",
        "loves her...'''\n",
        "\n",
        "word_tokens(story)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This obtains a list of word occurrences, but it has a few shortcomings:\n",
        "\n",
        "1.   There is an empty string at the end of the list because the string ends in a separator. Empty strings are however easy to remove.\n",
        "2.   Hyphenated words and abbreviations are split. This is not desirable.\n",
        "3.   Contractions such as *didn't* are split inappropriately, because apostrophes are part of the `\\W` category.\n",
        "\n",
        "A slightly better solution is to match sequences of \\w (alphanumerics) and a few other characters, using `findall` instead of `split`."
      ],
      "metadata": {
        "id": "D52kBA4D24qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_tokens (text):\n",
        "  return re.findall(r'[\\w\\'’-]+', text)\n",
        "\n",
        "tokens = word_tokens(story)\n",
        "tokens"
      ],
      "metadata": {
        "id": "L8wn-8LVg0RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better, but not perfect. It still does not handle abbreviations with periods, for example. It keeps contractions together, but does not distinguish between an apostrophe in a contraction and the same character used as a single quote around words, unless one actually uses different characters for these purposes. There may also be other character ambiguities, depending on the language. Writing a foolproof tokenizer is difficult, in part because there is no simple definition of what a word is."
      ],
      "metadata": {
        "id": "a-7E-cbbiXoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The set of word tokens, i.e. all unique tokens, provide the word *types*."
      ],
      "metadata": {
        "id": "X26JoCMhl1mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(tokens)"
      ],
      "metadata": {
        "id": "57WfR78rlzAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKVps5iZkNFc"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1.   Tokenization is often combined with normalization. Use the tokenizer on *casefolded* text.\n",
        "2.   This simple tokenizer does not make tokens for punctuation. Is this good or bad? \n",
        "3.   What would be other linguistically motivated ways of handling *didn't*?\n",
        "4.   Count the number of tokens.\n",
        "5.   How would you compute the word *types*, in other words, the set of *different* words in a text?\n",
        "6.   Using the palindrome test from an earlier notebook, check if a list of tokens is palindromic. Test with sentences like `'Fall leaves as soon as leaves fall.'`\n"
      ]
    }
  ]
}