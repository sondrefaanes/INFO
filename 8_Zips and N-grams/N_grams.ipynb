{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kbYAVzrCvAq"
      },
      "source": [
        "# N-grams with zip and with NLTK\n",
        "\n",
        "by Koenraad De Smedt at UiB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX37WY35C5eH"
      },
      "source": [
        "---\n",
        "*N-grams* are consecutive parts of a text broken up into *n* tokens (words or letters), such as the following.\n",
        "\n",
        "> ```\n",
        "Once upon a\n",
        "upon a time\n",
        "a time there\n",
        "time there was\n",
        "```\n",
        "\n",
        "Breaking up a text in n-grams can be useful for several NLP purposes, such as translation, error correction, finding collocations, document classification, etc.\n",
        "\n",
        "This notebook shows how with `zip`, we can easily compute n-grams as a list of tuples. Also the `ngram` function in NLTK is demonstrated.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with a word-tokenized text. See how we can take parts of the token list, starting at subsequent positions and running to the end. If you look at the lists in parallel, from top to bottom, you already get an impression of the n-grams."
      ],
      "metadata": {
        "id": "lWNGupnCmU_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = ['Once', 'upon', 'a', 'time', 'there', 'was', '...']\n",
        "\n",
        "print(tokens[0:])\n",
        "print(tokens[1:])\n",
        "print(tokens[2:])"
      ],
      "metadata": {
        "id": "JcZSVdKpmC1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrnJ8OyZ699M"
      },
      "source": [
        "With a comprehension that iterates over a range, we can include all such subsequent lists in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_bBMWv66kla"
      },
      "source": [
        "[tokens[i:] for i in range(3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M89O12r8jSL"
      },
      "source": [
        "If we unpack this list, and give the contained lists as arguments to  zip, we get a list of tuples which are word n-grams â€“ in this case, trigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzvkyoES7ITw"
      },
      "source": [
        "tri = zip(*[tokens[i:] for i in range(3)])\n",
        "#print(tri)\n",
        "[*tri]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to define a function that operates on a list of tokens and produces n-gram tuples. It has an extra argument for *n* with a default of 4. "
      ],
      "metadata": {
        "id": "SkjWG8slNF18"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUQshbBNCt1X"
      },
      "source": [
        "def n_grams (seq, n=3):\n",
        "  return zip(*[seq[i:] for i in range(n)])\n",
        "\n",
        "[*n_grams(tokens)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Override the default."
      ],
      "metadata": {
        "id": "GzzChYCabaoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[*n_grams(tokens, n=4)]"
      ],
      "metadata": {
        "id": "w4QKFHqYX-fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov6bDUM9ER6q"
      },
      "source": [
        "---\n",
        "## N-grams with NLTK\n",
        "\n",
        "Now that you understand how n-grams can be computed, let's look at NLTK which also provides a tool for n-grams. We must first tokenize, so we need a tokenizer and we make a slightly larger example text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGFJJk7OEQ-1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "story ='''Once upon a time, there was a princess called Buttercup. She had a\n",
        "farmhand called Westley; whenever she tells him to do something, he always\n",
        "answers: \"As you wish.\" At first she didn't realize he loves her, but\n",
        "eventually she realizes it and she loves him too! Westley leaves to seek his\n",
        "fortune overseas so they can marry. When his ship is attacked by the Dread\n",
        "Pirate Roberts, who is infamous for never leaving survivors, Westley is\n",
        "presumed dead...'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN7xUjlOD2pW"
      },
      "source": [
        "Let's test and show the first 10 n-grams with our own `n_gram` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uyfMC5cD499"
      },
      "source": [
        "ng = n_grams(word_tokenize(story))\n",
        "[*ng][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pjrr9MZYcVF"
      },
      "source": [
        "N-grams can be obtained in the same format by means of the `ngrams` function in NLTK. However, the second argument is obligatory; it has no default. The result is also a *generator* of tuples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQjDop43Yf11"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "ng2 = ngrams(word_tokenize(story), n=3)\n",
        "[*ng2][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HHKgIQhkNpj"
      },
      "source": [
        "###User interactions to set parameters\n",
        "\n",
        "Google Colab offers user interactions to set parameters. These are indicated as special comments with the `#@` characters. The following illustrates the use of a *slider* to choose the length of the n-grams. See the [forms example](https://colab.research.google.com/notebooks/forms.ipynb) for more possibilities. This may not work outside of Google Colab, but [IPywidgets](https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6) offers something similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkLFPwY2jvvC"
      },
      "source": [
        "#@title Choose the length of the n-grams\n",
        "N_gram_length = 2 #@param {type:\"slider\", min:2, max:5, step:1}\n",
        "ng2 = ngrams(word_tokenize(story), n=N_gram_length)\n",
        "[*ng2][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asfH5FN4FaHc"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1.   Compute the bigrams in `story`.\n",
        "2.   How many *different* bigrams are there in `story`?\n",
        "3.   Convert the n-gram tuples to strings with spaces, for instance, `'Once upon a'`.\n",
        "4.   See if you can make *character* n-grams from a text string. Compare the result with that in the notebook on Ranges."
      ]
    }
  ]
}